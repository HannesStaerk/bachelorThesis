\begin{frame}{Gliederung}
	\begin{itemize}
		\item Motivation
		\item Vorwissen
		\item Implementierung, Hardware, Software
		\item Datensatz
		\item Architekturen
		\item Architektur Experimente
		\item Latenter Raum Experimente
		\item Fazit, Wie kann es weiter gehen
	\end{itemize}
\end{frame}


\begin{frame}
	\begin{figure}
		\includegraphics[width=\textwidth]{images/figures/presentation/semantic_segmentation.jpg}
		\vspace*{15pt}\hbox{\scriptsize Credit:\thinspace{\small\itshape \cite{2017-audebert-segment}}}
	\end{figure}
\end{frame}

\begin{frame}
	\begin{figure}
		\includegraphics[width=\textwidth]{images/figures/presentation/multi-task-models.png}
		\vspace*{15pt}\hbox{\scriptsize Credit:\thinspace{\small\itshape \cite{2015-Riemer-multitask}}}
	\end{figure}
\end{frame}

\begin{frame}{Motivation}
	\begin{itemize}
		\item Trial-and-Error Multi-Task Architekturen
		\item Multi-Task Taxonomie
		\item Latente Informationen einzelner Schichten in Single-Task Modellen
		\item Latenten Raum eines Variational Autoencoders verstehen
	\end{itemize}
\end{frame}

\begin{frame}
	\begin{figure}
		\includegraphics[width=0.6\textwidth]{images/figures/presentation/autoencoder.png}
		\vspace*{15pt}\hbox{\scriptsize Credit:\thinspace{\small\itshape GRID INC}}
	\end{figure}
\end{frame}

\begin{frame}
	\begin{figure}
		\includegraphics[width=0.7\textwidth]{images/figures/presentation/variational_autoencoder.png}
		\vspace*{15pt}\hbox{\scriptsize Credit:\thinspace{\small\itshape GRID INC}}
	\end{figure}
\end{frame}

\begin{frame}{Implementierung, Hardware, Software}
	\begin{itemize}
		\item Python
		\item Tensorflow
		\item Container der Uni Hannover
		\item Machine-Learning Rechner der UniBw
	\end{itemize}
\end{frame}

\begin{frame}{Architektur Experimente}
	\begin{itemize}
		\item Anzahl von convolutional Schichten
		\item Anzahl von Filtern
		\item Kernel Größe
		\item Max/Average Pooling
	\end{itemize}
\end{frame}

\begin{frame}
	\begin{figure}
		\includegraphics[width=\textwidth]{images/figures/presentation/bad_reconstructions.jpg}
	\end{figure} 
\end{frame}

\begin{frame}
	\begin{figure}
		\includegraphics[width=\textwidth]{images/figures/presentation/num_filters_reconstructions.jpg}
	\end{figure} 
\end{frame}

\begin{frame}
	\begin{figure}
		\includegraphics[width=\textwidth]{images/figures/encoder_neural_network.png}
		\includegraphics[width=\textwidth]{images/figures/decoder_neural_network.png}
	\end{figure}
\end{frame}

\begin{frame}
	\begin{figure}
		\includegraphics[width=\textwidth]{images/figures/encoder_pooling.png}
	\end{figure}
\end{frame}

\begin{frame}
	\begin{figure}
		\includegraphics[width=0.8\textwidth]{images/figures/presentation/good_reconstructions.jpg}
	\end{figure}
\end{frame}







\begin{frame}{t-Stochastic-Neighbor-Embedding}
	\begin{itemize}
		\item Machine-Learning Verfahren zur Dimensions Reduktion
		\item Besonders gut geeignet für 
		\item Fokus auf Kontext von Punkten zu ihren Nachbarn
		\item 
	\end{itemize}
\end{frame}

	
\begin{frame}{Latenter Raum Experimente}
\end{frame}


\begin{frame}{Fazit und wie es weiter gehen kann}
\end{frame}