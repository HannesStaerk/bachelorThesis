\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\abx@aux@refcontext{nyt/global//global/global}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\@writefile{lof}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\@writefile{lot}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {1}Vorwort}{2}{section.1}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {2}Kurzfassung}{2}{section.2}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {3}Abstract}{2}{section.3}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {4}Introduction}{2}{section.4}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Topic Overview}{2}{subsection.4.1}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Research Questions}{2}{subsection.4.2}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {5}Background}{2}{section.5}\protected@file@percent }
\abx@aux@cite{2005-chrislb-artificial-neuron}
\abx@aux@segm{0}{0}{2005-chrislb-artificial-neuron}
\abx@aux@segm{0}{0}{2005-chrislb-artificial-neuron}
\abx@aux@cite{2017-geron-homl}
\abx@aux@segm{0}{0}{2017-geron-homl}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Remote Sensing}{3}{subsection.5.1}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Artificial Neuron}{3}{subsection.5.2}\protected@file@percent }
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces An artificial neuron. Image taken from \parencite {2005-chrislb-artificial-neuron}\relax }}{3}{figure.caption.2}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Artificial Neural Network}{3}{subsection.5.3}\protected@file@percent }
\abx@aux@cite{2013-glosser-ann}
\abx@aux@segm{0}{0}{2013-glosser-ann}
\abx@aux@segm{0}{0}{2013-glosser-ann}
\abx@aux@cite{2016-goodfellow-deep}
\abx@aux@segm{0}{0}{2016-goodfellow-deep}
\abx@aux@segm{0}{0}{2017-geron-homl}
\abx@aux@page{3}{4}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces An artificial neural network with three fully connected layers. Image taken from \parencite {2013-glosser-ann}\relax }}{4}{figure.caption.3}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{figure_fully_connected_nn}{{2}{4}{An artificial neural network with three fully connected layers. Image taken from \parencite {2013-glosser-ann}\relax }{figure.caption.3}{}}
\abx@aux@page{6}{4}
\abx@aux@page{7}{4}
\abx@aux@segm{0}{0}{2017-geron-homl}
\abx@aux@segm{0}{0}{2017-geron-homl}
\abx@aux@segm{0}{0}{2016-goodfellow-deep}
\abx@aux@cite{2012-krizhevsky-imagenet}
\abx@aux@segm{0}{0}{2012-krizhevsky-imagenet}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}Convolutional Neural Networks}{5}{subsection.5.4}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Pooling}{5}{section*.5}\protected@file@percent }
\abx@aux@page{10}{5}
\abx@aux@page{11}{5}
\abx@aux@cite{2015-Chervinskii-autoencoder}
\abx@aux@segm{0}{0}{2015-Chervinskii-autoencoder}
\abx@aux@segm{0}{0}{2015-Chervinskii-autoencoder}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces A convolutional network with 12 filters in the first convolutional layer and 7 filters in the second convolutional layer. Each filters can learn different features and an RGB image is depicted as the input here. Image taken from \parencite {2017-geron-homl}\relax }}{6}{figure.caption.4}\protected@file@percent }
\newlabel{figure_cnn_filter}{{3}{6}{A convolutional network with 12 filters in the first convolutional layer and 7 filters in the second convolutional layer. Each filters can learn different features and an RGB image is depicted as the input here. Image taken from \parencite {2017-geron-homl}\relax }{figure.caption.4}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {5.5}Autoencoders}{6}{subsection.5.5}\protected@file@percent }
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces An undercomplete autoencoder has a latent code $z$ whose dimension is smaller than the dimension of the input $x$. Image taken from \parencite {2015-Chervinskii-autoencoder}\relax }}{7}{figure.caption.6}\protected@file@percent }
\newlabel{figure_undercomplete_ae}{{4}{7}{An undercomplete autoencoder has a latent code $z$ whose dimension is smaller than the dimension of the input $x$. Image taken from \parencite {2015-Chervinskii-autoencoder}\relax }{figure.caption.6}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {5.6}Kullback-Leibler Divergence}{7}{subsection.5.6}\protected@file@percent }
\newlabel{KL-Divergence}{{5.6}{7}{Kullback-Leibler Divergence}{subsection.5.6}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.6.1}Entropy}{7}{subsubsection.5.6.1}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.6.2}Cross Entropy}{8}{subsubsection.5.6.2}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.6.3}Kullback-Leibler Divergence}{9}{subsubsection.5.6.3}\protected@file@percent }
\newlabel{eq1}{{3}{9}{Kullback-Leibler Divergence}{equation.5.3}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {5.7}Variational Autoencoders}{9}{subsection.5.7}\protected@file@percent }
\newlabel{vae_background}{{5.7}{9}{Variational Autoencoders}{subsection.5.7}{}}
\abx@aux@cite{2016-doersch-tutorial}
\abx@aux@segm{0}{0}{2016-doersch-tutorial}
\abx@aux@cite{2008-vanDerMaaten-visualizing}
\abx@aux@segm{0}{0}{2008-vanDerMaaten-visualizing}
\abx@aux@page{14}{10}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {5.8}t-Distributed Stochastic Neighbor Embedding}{10}{subsection.5.8}\protected@file@percent }
\newlabel{t-sne}{{5.8}{10}{t-Distributed Stochastic Neighbor Embedding}{subsection.5.8}{}}
\abx@aux@page{15}{10}
\newlabel{pij}{{4}{10}{t-Distributed Stochastic Neighbor Embedding}{equation.5.4}{}}
\newlabel{qij}{{5}{10}{t-Distributed Stochastic Neighbor Embedding}{equation.5.5}{}}
\abx@aux@cite{2015-springenberg-striving}
\abx@aux@segm{0}{0}{2015-springenberg-striving}
\abx@aux@cite{2016-mishkin-systematic}
\abx@aux@segm{0}{0}{2016-mishkin-systematic}
\abx@aux@cite{2017-klambauer-selu}
\abx@aux@segm{0}{0}{2017-klambauer-selu}
\abx@aux@cite{2018-Pedamonti-comparison}
\abx@aux@segm{0}{0}{2018-Pedamonti-comparison}
\abx@aux@cite{2015-theis-generative}
\abx@aux@segm{0}{0}{2015-theis-generative}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {6}Related Work}{11}{section.6}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}General and Convolutional Network Architecture}{11}{subsection.6.1}\protected@file@percent }
\newlabel{related_work_general_architecture}{{6.1}{11}{General and Convolutional Network Architecture}{subsection.6.1}{}}
\abx@aux@page{16}{11}
\abx@aux@page{17}{11}
\abx@aux@page{18}{11}
\abx@aux@page{19}{11}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Variational Autoencoders and Generative Networks}{11}{subsection.6.2}\protected@file@percent }
\abx@aux@page{20}{11}
\abx@aux@cite{1995-rossum-python}
\abx@aux@segm{0}{0}{1995-rossum-python}
\abx@aux@cite{2015-martin-tensorflow}
\abx@aux@segm{0}{0}{2015-martin-tensorflow}
\abx@aux@cite{2011-pedregosa-scikit}
\abx@aux@segm{0}{0}{2011-pedregosa-scikit}
\abx@aux@cite{2019-bosch-semantic}
\abx@aux@segm{0}{0}{2019-bosch-semantic}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {7}Methodology}{12}{section.7}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}Environment}{12}{subsection.7.1}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.1.1}Hardware}{12}{subsubsection.7.1.1}\protected@file@percent }
\newlabel{hardware}{{7.1.1}{12}{Hardware}{subsubsection.7.1.1}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Personal Computer}{12}{section*.7}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Remote Machine}{12}{section*.8}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.1.2}Software}{12}{subsubsection.7.1.2}\protected@file@percent }
\abx@aux@page{21}{12}
\abx@aux@page{22}{12}
\abx@aux@page{23}{12}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {7.2}Datasets}{13}{subsection.7.2}\protected@file@percent }
\newlabel{datasets}{{7.2}{13}{Datasets}{subsection.7.2}{}}
\abx@aux@page{24}{13}
\abx@aux@segm{0}{0}{2017-geron-homl}
\abx@aux@segm{0}{0}{2017-klambauer-selu}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {7.3}Architecture}{14}{subsection.7.3}\protected@file@percent }
\newlabel{architecture}{{7.3}{14}{Architecture}{subsection.7.3}{}}
\abx@aux@page{25}{14}
\abx@aux@page{26}{14}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.3.1}Pure Convolutional Architecture}{15}{subsubsection.7.3.1}\protected@file@percent }
\newlabel{section_pure_convolutional_architecture}{{7.3.1}{15}{Pure Convolutional Architecture}{subsubsection.7.3.1}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Encoder network architecture with an encoding size of 1024. The kernel size is $3\times 3$ in every layer. The reparameterization trick in the last layer refers to the sampling as described in section \ref  {vae_background}. Dimensions are specified as $depth@height\times width$.\relax }}{15}{figure.caption.9}\protected@file@percent }
\newlabel{figure_pure_convolutional_encoder}{{5}{15}{Encoder network architecture with an encoding size of 1024. The kernel size is $3\times 3$ in every layer. The reparameterization trick in the last layer refers to the sampling as described in section \ref {vae_background}. Dimensions are specified as $depth@height\times width$.\relax }{figure.caption.9}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.3.2}Max Pooling Architecture}{15}{subsubsection.7.3.2}\protected@file@percent }
\abx@aux@segm{0}{0}{2008-vanDerMaaten-visualizing}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Decoder network architecture with an encoding size of 1024. The kernel size is $3\times 3$ in every layer. The last deconvolutional layer has a sigmoid activation function. Dimensions are specified as $depth@height\times width$.\relax }}{16}{figure.caption.10}\protected@file@percent }
\newlabel{figure_decoder}{{6}{16}{Decoder network architecture with an encoding size of 1024. The kernel size is $3\times 3$ in every layer. The last deconvolutional layer has a sigmoid activation function. Dimensions are specified as $depth@height\times width$.\relax }{figure.caption.10}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Part of the encoder network architecture with pooling and an encoding size of 1024. The convolutional kernel always has size $3\times 3$. The pooling window always has size $2\times 2$. The rest of the encoder that generates the latent code is the same as in the fully convolutional encoder \ref  {section_pure_convolutional_architecture}. Dimensions are specified as $depth@height\times width$.\relax }}{16}{figure.caption.11}\protected@file@percent }
\newlabel{figure_encoder_pooling}{{7}{16}{Part of the encoder network architecture with pooling and an encoding size of 1024. The convolutional kernel always has size $3\times 3$. The pooling window always has size $2\times 2$. The rest of the encoder that generates the latent code is the same as in the fully convolutional encoder \ref {section_pure_convolutional_architecture}. Dimensions are specified as $depth@height\times width$.\relax }{figure.caption.11}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {7.4}Understanding Latent Space}{16}{subsection.7.4}\protected@file@percent }
\abx@aux@page{27}{17}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {8}Experiments}{17}{section.8}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {8.1}Variational Autoencoder Architecture}{17}{subsection.8.1}\protected@file@percent }
\newlabel{architecture_experiments}{{8.1}{17}{Variational Autoencoder Architecture}{subsection.8.1}{}}
\newlabel{table_encoder_num_conv}{{\caption@xref {table_encoder_num_conv}{ on input line 41}}{18}{Number of Convolutions}{table.caption.12}{}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces The layers of the encoder of test $4$ up to the vector that the two dense layers use to produce the means and standard deviations for the latent code. Test $x$ only contains the layers up to the layer with $x$ in the \textit  {Test} column and the layers with \textit  {all}. The output of the \textit  {Flatten} layer varies depending on the test.\relax }}{18}{table.caption.12}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.1.1}Number of Convolutions}{18}{subsubsection.8.1.1}\protected@file@percent }
\newlabel{section_number_of_convolutions_experiment}{{8.1.1}{18}{Number of Convolutions}{subsubsection.8.1.1}{}}
\newlabel{figure_learning_curves_1}{{8.1.1}{18}{Number of Convolutions}{figure.caption.14}{}}
\newlabel{table_mae_1}{{8.1.1}{18}{Number of Convolutions}{table.caption.15}{}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces The layers of the decoder of test $4$. Test $x$ only contains the layers after the layer with the number $x$ in the \textit  {Test} column and the layers with \textit  {all}. The outputs of the \textit  {Dense} and \textit  {Reshape} layers vary depending on the test.\relax }}{19}{table.caption.13}\protected@file@percent }
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces There is a graph for each test architecture. Each graph depicts the mean MAEs between input and reconstructions across all iterations in an epoch of training.\relax }}{19}{figure.caption.14}\protected@file@percent }
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces For each test architecture the table shows the mean of the MAEs across all iterations of the last epoch, the mean loss across all iterations of the last epoch and the time it took to train the model on the personal computer as specified in section \ref  {hardware}. Notice that the loss is not the same as MAE since it additionally takes the Kullback-Leibler divergence into account.\relax }}{19}{table.caption.15}\protected@file@percent }
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces The left column shows original images taken from a validation set that the VAE has not seen during training. The other columns show the reconstructions of the VAE test architectures.\relax }}{20}{figure.caption.16}\protected@file@percent }
\newlabel{figure_reconstructions_1}{{8.1.1}{20}{Number of Convolutions}{figure.caption.16}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.1.2}Number of Filters}{20}{subsubsection.8.1.2}\protected@file@percent }
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces The layers of the encoder of test $4$ up to the vector that the two dense layers use to produce the means and standard deviations for the latent code. Test $x$ only contains the layers up to the layer with $x$ in the \textit  {Test} column and the layers with \textit  {all}. The output of the \textit  {Flatten} layer varies depending on the test.\relax }}{21}{table.caption.17}\protected@file@percent }
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces The layers of the decoder of test $4$. Test $x$ only contains the layers after the layer with the number $x$ in the \textit  {Test} column and the layers with \textit  {all}. The outputs of the \textit  {Dense} and \textit  {Reshape} layers vary depending on the test.\relax }}{21}{table.caption.18}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.1.3}Kernel Size}{21}{subsubsection.8.1.3}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.1.4}Pooling}{21}{subsubsection.8.1.4}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {8.2}Latent Space}{21}{subsection.8.2}\protected@file@percent }
\newlabel{latent_space_experiments}{{8.2}{21}{Latent Space}{subsection.8.2}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {9}Conclusion and Future Work}{21}{section.9}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {9.1}Conclusion}{21}{subsection.9.1}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {9.2}Future Work}{21}{subsection.9.2}\protected@file@percent }
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces There is a graph for each test architecture. Each graph depicts the mean MAEs between input and reconstructions across all iterations in an epoch of training.\relax }}{22}{figure.caption.19}\protected@file@percent }
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces For each test architecture the table shows the mean of the MAEs across all iterations of the last epoch, the mean loss across all iterations of the last epoch and the time it took to train the model on the personal computer as described in section \ref  {hardware}. Notice that the loss is not the same as MAE since it additionally takes the Kullback-Leibler divergence into account.\relax }}{22}{table.caption.20}\protected@file@percent }
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces The left column shows original images taken from a validation set that the VAE has not seen during training. The other columns show the reconstructions of the VAE test architectures.\relax }}{22}{figure.caption.21}\protected@file@percent }
\abx@aux@refcontextdefaultsdone
\abx@aux@defaultrefcontext{0}{2019-bosch-semantic}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{2015-Chervinskii-autoencoder}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{2005-chrislb-artificial-neuron}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{2016-doersch-tutorial}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{2017-geron-homl}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{2013-glosser-ann}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{2016-goodfellow-deep}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{2017-klambauer-selu}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{2012-krizhevsky-imagenet}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{2008-vanDerMaaten-visualizing}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{2015-martin-tensorflow}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{2016-mishkin-systematic}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{2018-Pedamonti-comparison}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{2011-pedregosa-scikit}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{1995-rossum-python}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{2015-springenberg-striving}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{2015-theis-generative}{nyt/global//global/global}
\abx@aux@page{28}{23}
\abx@aux@page{29}{23}
\abx@aux@page{30}{23}
\abx@aux@page{31}{23}
\abx@aux@page{32}{23}
\abx@aux@page{33}{23}
\abx@aux@page{34}{23}
\abx@aux@page{35}{23}
\abx@aux@page{36}{23}
\abx@aux@page{37}{23}
\abx@aux@page{38}{23}
\abx@aux@page{39}{23}
\abx@aux@page{40}{23}
\abx@aux@page{41}{23}
\abx@aux@page{42}{23}
\abx@aux@page{43}{23}
\abx@aux@page{44}{23}
