\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\abx@aux@refcontext{nyt/global//global/global}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\@writefile{lof}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\@writefile{lot}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {1}Vorwort}{2}{section.1}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {2}Kurzfassung}{2}{section.2}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {3}Abstract}{2}{section.3}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {4}Introduction}{2}{section.4}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Topic Overview}{2}{subsection.4.1}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Research Questions}{2}{subsection.4.2}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {5}Background}{2}{section.5}\protected@file@percent }
\abx@aux@cite{2005-chrislb-artificial-neuron}
\abx@aux@segm{0}{0}{2005-chrislb-artificial-neuron}
\abx@aux@segm{0}{0}{2005-chrislb-artificial-neuron}
\abx@aux@cite{2017-geron-homl}
\abx@aux@segm{0}{0}{2017-geron-homl}
\abx@aux@cite{2013-glosser-ann}
\abx@aux@segm{0}{0}{2013-glosser-ann}
\abx@aux@segm{0}{0}{2013-glosser-ann}
\abx@aux@cite{2016-goodfellow-deep}
\abx@aux@segm{0}{0}{2016-goodfellow-deep}
\abx@aux@segm{0}{0}{2017-geron-homl}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Remote Sensing Imagery}{3}{subsection.5.1}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Artificial Neuron}{3}{subsection.5.2}\protected@file@percent }
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces An artificial neuron. Image taken from \parencite {2005-chrislb-artificial-neuron}\relax }}{3}{figure.caption.2}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Artificial Neural Network}{3}{subsection.5.3}\protected@file@percent }
\abx@aux@page{3}{3}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces An artificial neural network with three fully connected layers. Image taken from \parencite {2013-glosser-ann}\relax }}{4}{figure.caption.3}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{figure_fully_connected_nn}{{2}{4}{An artificial neural network with three fully connected layers. Image taken from \parencite {2013-glosser-ann}\relax }{figure.caption.3}{}}
\abx@aux@page{6}{4}
\abx@aux@page{7}{4}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}Convolutional Neural Networks}{4}{subsection.5.4}\protected@file@percent }
\abx@aux@segm{0}{0}{2017-geron-homl}
\abx@aux@segm{0}{0}{2017-geron-homl}
\abx@aux@segm{0}{0}{2016-goodfellow-deep}
\abx@aux@cite{2012-krizhevsky-imagenet}
\abx@aux@segm{0}{0}{2012-krizhevsky-imagenet}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces A convolutional network with 12 filters in the first convolutional layer and 7 filters in the second convolutional layer. Each filters can learn different features and an RGB image is depicted as the input here. Image taken from \parencite {2017-geron-homl}\relax }}{5}{figure.caption.4}\protected@file@percent }
\newlabel{figure_cnn_filter}{{3}{5}{A convolutional network with 12 filters in the first convolutional layer and 7 filters in the second convolutional layer. Each filters can learn different features and an RGB image is depicted as the input here. Image taken from \parencite {2017-geron-homl}\relax }{figure.caption.4}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Pooling}{5}{section*.5}\protected@file@percent }
\abx@aux@page{10}{5}
\abx@aux@page{11}{5}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {5.5}Autoencoders}{5}{subsection.5.5}\protected@file@percent }
\abx@aux@cite{2015-Chervinskii-autoencoder}
\abx@aux@segm{0}{0}{2015-Chervinskii-autoencoder}
\abx@aux@segm{0}{0}{2015-Chervinskii-autoencoder}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces An undercomplete autoencoder has a latent code $z$ whose dimension is smaller than the dimension of the input $x$. Image taken from \parencite {2015-Chervinskii-autoencoder}\relax }}{6}{figure.caption.6}\protected@file@percent }
\newlabel{figure_undercomplete_ae}{{4}{6}{An undercomplete autoencoder has a latent code $z$ whose dimension is smaller than the dimension of the input $x$. Image taken from \parencite {2015-Chervinskii-autoencoder}\relax }{figure.caption.6}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {5.6}Kullback-Leibler Divergence}{6}{subsection.5.6}\protected@file@percent }
\newlabel{KL-Divergence}{{5.6}{6}{Kullback-Leibler Divergence}{subsection.5.6}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.6.1}Entropy}{6}{subsubsection.5.6.1}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.6.2}Cross Entropy}{8}{subsubsection.5.6.2}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.6.3}Kullback-Leibler Divergence}{8}{subsubsection.5.6.3}\protected@file@percent }
\newlabel{eq1}{{3}{8}{Kullback-Leibler Divergence}{equation.5.3}{}}
\abx@aux@cite{2016-doersch-tutorial}
\abx@aux@segm{0}{0}{2016-doersch-tutorial}
\abx@aux@cite{2008-vanDerMaaten-visualizing}
\abx@aux@segm{0}{0}{2008-vanDerMaaten-visualizing}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {5.7}Variational Autoencoders}{9}{subsection.5.7}\protected@file@percent }
\newlabel{vae_background}{{5.7}{9}{Variational Autoencoders}{subsection.5.7}{}}
\abx@aux@page{14}{9}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {5.8}t-Distributed Stochastic Neighbor Embedding}{9}{subsection.5.8}\protected@file@percent }
\newlabel{t-sne}{{5.8}{9}{t-Distributed Stochastic Neighbor Embedding}{subsection.5.8}{}}
\abx@aux@page{15}{9}
\abx@aux@cite{2015-springenberg-striving}
\abx@aux@segm{0}{0}{2015-springenberg-striving}
\abx@aux@cite{2016-mishkin-systematic}
\abx@aux@segm{0}{0}{2016-mishkin-systematic}
\abx@aux@cite{2017-klambauer-selu}
\abx@aux@segm{0}{0}{2017-klambauer-selu}
\newlabel{pij}{{4}{10}{t-Distributed Stochastic Neighbor Embedding}{equation.5.4}{}}
\newlabel{qij}{{5}{10}{t-Distributed Stochastic Neighbor Embedding}{equation.5.5}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {6}Related Work}{10}{section.6}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}General and Convolutional Network Architecture}{10}{subsection.6.1}\protected@file@percent }
\newlabel{related_work_general_architecture}{{6.1}{10}{General and Convolutional Network Architecture}{subsection.6.1}{}}
\abx@aux@page{16}{10}
\abx@aux@page{17}{10}
\abx@aux@page{18}{10}
\abx@aux@cite{2018-Pedamonti-comparison}
\abx@aux@segm{0}{0}{2018-Pedamonti-comparison}
\abx@aux@cite{2015-theis-generative}
\abx@aux@segm{0}{0}{2015-theis-generative}
\abx@aux@page{19}{11}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Variational Autoencoders and Generative Networks}{11}{subsection.6.2}\protected@file@percent }
\abx@aux@page{20}{11}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {7}Methodology}{11}{section.7}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}Environment}{11}{subsection.7.1}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.1.1}Hardware}{11}{subsubsection.7.1.1}\protected@file@percent }
\abx@aux@cite{1995-rossum-python}
\abx@aux@segm{0}{0}{1995-rossum-python}
\abx@aux@cite{2015-martin-tensorflow}
\abx@aux@segm{0}{0}{2015-martin-tensorflow}
\abx@aux@cite{2011-pedregosa-scikit}
\abx@aux@segm{0}{0}{2011-pedregosa-scikit}
\abx@aux@cite{2019-bosch-semantic}
\abx@aux@segm{0}{0}{2019-bosch-semantic}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Personal Computer}{12}{section*.7}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Remote Machine}{12}{section*.8}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.1.2}Software}{12}{subsubsection.7.1.2}\protected@file@percent }
\abx@aux@page{21}{12}
\abx@aux@page{22}{12}
\abx@aux@page{23}{12}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {7.2}Datasets}{12}{subsection.7.2}\protected@file@percent }
\newlabel{datasets}{{7.2}{12}{Datasets}{subsection.7.2}{}}
\abx@aux@page{24}{12}
\abx@aux@segm{0}{0}{2017-geron-homl}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {7.3}Architecture}{13}{subsection.7.3}\protected@file@percent }
\newlabel{architecture}{{7.3}{13}{Architecture}{subsection.7.3}{}}
\abx@aux@page{25}{13}
\abx@aux@segm{0}{0}{2017-klambauer-selu}
\abx@aux@page{26}{14}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.3.1}Pure Convolutional Architecture}{14}{subsubsection.7.3.1}\protected@file@percent }
\newlabel{section_pure_convolutional_architecture}{{7.3.1}{14}{Pure Convolutional Architecture}{subsubsection.7.3.1}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.3.2}Max Pooling Architecture}{14}{subsubsection.7.3.2}\protected@file@percent }
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Encoder network architecture with an encoding size of 1024. The kernel size is $3\times 3$ in every layer. The reparameterization trick in the last layer refers to the sampling as described in section \ref  {vae_background}. Dimensions are specified as $depth@height\times width$.\relax }}{15}{figure.caption.9}\protected@file@percent }
\newlabel{figure_pure_convolutional_encoder}{{5}{15}{Encoder network architecture with an encoding size of 1024. The kernel size is $3\times 3$ in every layer. The reparameterization trick in the last layer refers to the sampling as described in section \ref {vae_background}. Dimensions are specified as $depth@height\times width$.\relax }{figure.caption.9}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Decoder network architecture with an encoding size of 1024. The kernel size is $3\times 3$ in every layer. The last deconvolutional layer has a sigmoid activation function. Dimensions are specified as $depth@height\times width$.\relax }}{15}{figure.caption.10}\protected@file@percent }
\newlabel{figure_decoder}{{6}{15}{Decoder network architecture with an encoding size of 1024. The kernel size is $3\times 3$ in every layer. The last deconvolutional layer has a sigmoid activation function. Dimensions are specified as $depth@height\times width$.\relax }{figure.caption.10}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {7.4}Understanding Latent Space}{15}{subsection.7.4}\protected@file@percent }
\abx@aux@segm{0}{0}{2008-vanDerMaaten-visualizing}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Part of the encoder network architecture with pooling and an encoding size of 1024. The convolutional kernel always has size $3\times 3$. The pooling window always has size $2\times 2$. The rest of the encoder that generates the latent code is the same as in the fully convolutional encoder \ref  {section_pure_convolutional_architecture}. Dimensions are specified as $depth@height\times width$.\relax }}{16}{figure.caption.11}\protected@file@percent }
\newlabel{figure_encoder_pooling}{{7}{16}{Part of the encoder network architecture with pooling and an encoding size of 1024. The convolutional kernel always has size $3\times 3$. The pooling window always has size $2\times 2$. The rest of the encoder that generates the latent code is the same as in the fully convolutional encoder \ref {section_pure_convolutional_architecture}. Dimensions are specified as $depth@height\times width$.\relax }{figure.caption.11}{}}
\abx@aux@page{27}{16}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {8}Experiments}{17}{section.8}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {8.1}Variational Autoencoder Architecture}{17}{subsection.8.1}\protected@file@percent }
\newlabel{architecture_experiments}{{8.1}{17}{Variational Autoencoder Architecture}{subsection.8.1}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {8.2}Latent Space}{17}{subsection.8.2}\protected@file@percent }
\newlabel{latent_space_experiments}{{8.2}{17}{Latent Space}{subsection.8.2}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {9}Conclusion and Future Work}{17}{section.9}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {9.1}Conclusion}{17}{subsection.9.1}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {9.2}Future Work}{17}{subsection.9.2}\protected@file@percent }
\abx@aux@refcontextdefaultsdone
\abx@aux@defaultrefcontext{0}{2019-bosch-semantic}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{2015-Chervinskii-autoencoder}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{2005-chrislb-artificial-neuron}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{2016-doersch-tutorial}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{2017-geron-homl}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{2013-glosser-ann}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{2016-goodfellow-deep}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{2017-klambauer-selu}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{2012-krizhevsky-imagenet}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{2008-vanDerMaaten-visualizing}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{2015-martin-tensorflow}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{2016-mishkin-systematic}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{2018-Pedamonti-comparison}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{2011-pedregosa-scikit}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{1995-rossum-python}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{2015-springenberg-striving}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{2015-theis-generative}{nyt/global//global/global}
\abx@aux@page{28}{18}
\abx@aux@page{29}{18}
\abx@aux@page{30}{18}
\abx@aux@page{31}{18}
\abx@aux@page{32}{18}
\abx@aux@page{33}{18}
\abx@aux@page{34}{18}
\abx@aux@page{35}{18}
\abx@aux@page{36}{18}
\abx@aux@page{37}{18}
\abx@aux@page{38}{18}
\abx@aux@page{39}{18}
\abx@aux@page{40}{18}
\abx@aux@page{41}{18}
\abx@aux@page{42}{18}
\abx@aux@page{43}{18}
\abx@aux@page{44}{18}
