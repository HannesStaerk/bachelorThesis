\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\abx@aux@refcontext{nyt/global//global/global}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\@writefile{lof}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\@writefile{lot}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\abx@aux@cite{2017-shelhamer-fully}
\abx@aux@segm{0}{0}{2017-shelhamer-fully}
\abx@aux@cite{2016-jegou-the}
\abx@aux@segm{0}{0}{2016-jegou-the}
\abx@aux@cite{2018-kendall-multi}
\abx@aux@segm{0}{0}{2018-kendall-multi}
\abx@aux@cite{2019-schmitz-semantic}
\abx@aux@segm{0}{0}{2019-schmitz-semantic}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {1}Vorwort}{2}{section.1}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {2}Kurzfassung}{2}{section.2}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {3}Abstract}{2}{section.3}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {4}Introduction}{2}{section.4}\protected@file@percent }
\abx@aux@page{1}{2}
\abx@aux@page{2}{2}
\abx@aux@page{3}{2}
\abx@aux@page{4}{2}
\abx@aux@segm{0}{0}{2018-kendall-multi}
\abx@aux@segm{0}{0}{2019-schmitz-semantic}
\abx@aux@cite{2019-vandenhende-branched}
\abx@aux@segm{0}{0}{2019-vandenhende-branched}
\abx@aux@page{5}{3}
\abx@aux@page{6}{3}
\abx@aux@page{7}{3}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {5}Background}{3}{section.5}\protected@file@percent }
\abx@aux@cite{2005-chrislb-artificial-neuron}
\abx@aux@segm{0}{0}{2005-chrislb-artificial-neuron}
\abx@aux@segm{0}{0}{2005-chrislb-artificial-neuron}
\abx@aux@cite{2017-geron-homl}
\abx@aux@segm{0}{0}{2017-geron-homl}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Remote Sensing}{4}{subsection.5.1}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Artificial Neuron}{4}{subsection.5.2}\protected@file@percent }
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces An artificial neuron. Image taken from \parencite {2005-chrislb-artificial-neuron}\relax }}{4}{figure.caption.2}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Artificial Neural Network}{4}{subsection.5.3}\protected@file@percent }
\abx@aux@cite{2013-glosser-ann}
\abx@aux@segm{0}{0}{2013-glosser-ann}
\abx@aux@segm{0}{0}{2013-glosser-ann}
\abx@aux@cite{2016-goodfellow-deep}
\abx@aux@segm{0}{0}{2016-goodfellow-deep}
\abx@aux@segm{0}{0}{2017-geron-homl}
\abx@aux@page{10}{5}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces An artificial neural network with three fully connected layers. Image taken from \parencite {2013-glosser-ann}\relax }}{5}{figure.caption.3}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{figure_fully_connected_nn}{{2}{5}{An artificial neural network with three fully connected layers. Image taken from \parencite {2013-glosser-ann}\relax }{figure.caption.3}{}}
\abx@aux@page{13}{5}
\abx@aux@segm{0}{0}{2017-geron-homl}
\abx@aux@segm{0}{0}{2017-geron-homl}
\abx@aux@segm{0}{0}{2016-goodfellow-deep}
\abx@aux@cite{2012-krizhevsky-imagenet}
\abx@aux@segm{0}{0}{2012-krizhevsky-imagenet}
\abx@aux@page{14}{6}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}Convolutional Neural Networks}{6}{subsection.5.4}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Pooling}{6}{section*.5}\protected@file@percent }
\abx@aux@page{17}{6}
\abx@aux@page{18}{6}
\abx@aux@cite{2015-Chervinskii-autoencoder}
\abx@aux@segm{0}{0}{2015-Chervinskii-autoencoder}
\abx@aux@segm{0}{0}{2015-Chervinskii-autoencoder}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces A convolutional network with 12 filters in the first convolutional layer and 7 filters in the second convolutional layer. Each filters can learn different features and an RGB image is depicted as the input here. Image taken from \parencite {2017-geron-homl}\relax }}{7}{figure.caption.4}\protected@file@percent }
\newlabel{figure_cnn_filter}{{3}{7}{A convolutional network with 12 filters in the first convolutional layer and 7 filters in the second convolutional layer. Each filters can learn different features and an RGB image is depicted as the input here. Image taken from \parencite {2017-geron-homl}\relax }{figure.caption.4}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {5.5}Autoencoders}{7}{subsection.5.5}\protected@file@percent }
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces An undercomplete autoencoder has a latent code $z$ whose dimension is smaller than the dimension of the input $x$. Image taken from \parencite {2015-Chervinskii-autoencoder}\relax }}{8}{figure.caption.6}\protected@file@percent }
\newlabel{figure_undercomplete_ae}{{4}{8}{An undercomplete autoencoder has a latent code $z$ whose dimension is smaller than the dimension of the input $x$. Image taken from \parencite {2015-Chervinskii-autoencoder}\relax }{figure.caption.6}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {5.6}Kullback-Leibler Divergence}{8}{subsection.5.6}\protected@file@percent }
\newlabel{KL-Divergence}{{5.6}{8}{Kullback-Leibler Divergence}{subsection.5.6}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.6.1}Entropy}{8}{subsubsection.5.6.1}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.6.2}Cross Entropy}{9}{subsubsection.5.6.2}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.6.3}Kullback-Leibler Divergence}{10}{subsubsection.5.6.3}\protected@file@percent }
\newlabel{eq1}{{3}{10}{Kullback-Leibler Divergence}{equation.5.3}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {5.7}Variational Autoencoders}{10}{subsection.5.7}\protected@file@percent }
\newlabel{vae_background}{{5.7}{10}{Variational Autoencoders}{subsection.5.7}{}}
\abx@aux@cite{2016-doersch-tutorial}
\abx@aux@segm{0}{0}{2016-doersch-tutorial}
\abx@aux@cite{2008-vanDerMaaten-visualizing}
\abx@aux@segm{0}{0}{2008-vanDerMaaten-visualizing}
\abx@aux@page{21}{11}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {5.8}t-Distributed Stochastic Neighbor Embedding}{11}{subsection.5.8}\protected@file@percent }
\newlabel{t-sne}{{5.8}{11}{t-Distributed Stochastic Neighbor Embedding}{subsection.5.8}{}}
\abx@aux@page{22}{11}
\newlabel{pij}{{4}{11}{t-Distributed Stochastic Neighbor Embedding}{equation.5.4}{}}
\newlabel{qij}{{5}{11}{t-Distributed Stochastic Neighbor Embedding}{equation.5.5}{}}
\abx@aux@cite{2015-springenberg-striving}
\abx@aux@segm{0}{0}{2015-springenberg-striving}
\abx@aux@cite{2016-mishkin-systematic}
\abx@aux@segm{0}{0}{2016-mishkin-systematic}
\abx@aux@cite{2017-klambauer-selu}
\abx@aux@segm{0}{0}{2017-klambauer-selu}
\abx@aux@cite{2018-Pedamonti-comparison}
\abx@aux@segm{0}{0}{2018-Pedamonti-comparison}
\abx@aux@cite{2015-theis-generative}
\abx@aux@segm{0}{0}{2015-theis-generative}
\abx@aux@cite{2018-zhang-visual}
\abx@aux@segm{0}{0}{2018-zhang-visual}
\abx@aux@cite{2018-montavon-methods}
\abx@aux@segm{0}{0}{2018-montavon-methods}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {6}Related Work}{12}{section.6}\protected@file@percent }
\newlabel{related_work_general_architecture}{{6}{12}{Related Work}{section.6}{}}
\abx@aux@page{23}{12}
\abx@aux@page{24}{12}
\abx@aux@page{25}{12}
\abx@aux@page{26}{12}
\abx@aux@page{27}{12}
\abx@aux@page{28}{12}
\abx@aux@page{29}{12}
\abx@aux@cite{1995-rossum-python}
\abx@aux@segm{0}{0}{1995-rossum-python}
\abx@aux@cite{2015-martin-tensorflow}
\abx@aux@segm{0}{0}{2015-martin-tensorflow}
\abx@aux@cite{2011-pedregosa-scikit}
\abx@aux@segm{0}{0}{2011-pedregosa-scikit}
\abx@aux@cite{2019-bosch-semantic}
\abx@aux@segm{0}{0}{2019-bosch-semantic}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {7}Methodology}{13}{section.7}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}Environment}{13}{subsection.7.1}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.1.1}Hardware}{13}{subsubsection.7.1.1}\protected@file@percent }
\newlabel{hardware}{{7.1.1}{13}{Hardware}{subsubsection.7.1.1}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Personal Computer}{13}{section*.7}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Remote Machine}{13}{section*.8}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.1.2}Software}{13}{subsubsection.7.1.2}\protected@file@percent }
\abx@aux@page{30}{13}
\abx@aux@page{31}{13}
\abx@aux@page{32}{13}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {7.2}Datasets}{14}{subsection.7.2}\protected@file@percent }
\newlabel{datasets}{{7.2}{14}{Datasets}{subsection.7.2}{}}
\abx@aux@page{33}{14}
\abx@aux@segm{0}{0}{2017-geron-homl}
\abx@aux@segm{0}{0}{2017-klambauer-selu}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {7.3}Architecture}{15}{subsection.7.3}\protected@file@percent }
\newlabel{architecture}{{7.3}{15}{Architecture}{subsection.7.3}{}}
\abx@aux@page{34}{15}
\abx@aux@page{35}{15}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.3.1}Fully Convolutional Architecture}{16}{subsubsection.7.3.1}\protected@file@percent }
\newlabel{section_pure_convolutional_architecture}{{7.3.1}{16}{Fully Convolutional Architecture}{subsubsection.7.3.1}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Encoder network architecture with an encoding size of 1024. The kernel size is $3\times 3$ in every layer. The reparameterization trick in the last layer refers to the sampling as described in section \ref  {vae_background}. Dimensions are specified as $depth@height\times width$.\relax }}{16}{figure.caption.9}\protected@file@percent }
\newlabel{figure_pure_convolutional_encoder}{{5}{16}{Encoder network architecture with an encoding size of 1024. The kernel size is $3\times 3$ in every layer. The reparameterization trick in the last layer refers to the sampling as described in section \ref {vae_background}. Dimensions are specified as $depth@height\times width$.\relax }{figure.caption.9}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.3.2}Max Pooling Architecture}{16}{subsubsection.7.3.2}\protected@file@percent }
\abx@aux@segm{0}{0}{2008-vanDerMaaten-visualizing}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Decoder network architecture with an encoding size of 1024. The kernel size is $3\times 3$ in every layer. The last deconvolutional layer has a sigmoid activation function. Dimensions are specified as $depth@height\times width$.\relax }}{17}{figure.caption.10}\protected@file@percent }
\newlabel{figure_decoder}{{6}{17}{Decoder network architecture with an encoding size of 1024. The kernel size is $3\times 3$ in every layer. The last deconvolutional layer has a sigmoid activation function. Dimensions are specified as $depth@height\times width$.\relax }{figure.caption.10}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Part of the encoder network architecture with pooling and an encoding size of 1024. The convolutional kernel always has size $3\times 3$. The pooling window always has size $2\times 2$. The rest of the encoder that generates the latent code is the same as in the fully convolutional encoder \ref  {section_pure_convolutional_architecture}. Dimensions are specified as $depth@height\times width$.\relax }}{17}{figure.caption.11}\protected@file@percent }
\newlabel{figure_encoder_pooling}{{7}{17}{Part of the encoder network architecture with pooling and an encoding size of 1024. The convolutional kernel always has size $3\times 3$. The pooling window always has size $2\times 2$. The rest of the encoder that generates the latent code is the same as in the fully convolutional encoder \ref {section_pure_convolutional_architecture}. Dimensions are specified as $depth@height\times width$.\relax }{figure.caption.11}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {7.4}Understanding Latent Space}{17}{subsection.7.4}\protected@file@percent }
\newlabel{section_understanding_latent_space}{{7.4}{17}{Understanding Latent Space}{subsection.7.4}{}}
\abx@aux@page{36}{18}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {8}Experiments}{18}{section.8}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {8.1}Variational Autoencoder Architecture}{18}{subsection.8.1}\protected@file@percent }
\newlabel{architecture_experiments}{{8.1}{18}{Variational Autoencoder Architecture}{subsection.8.1}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.1.1}Number of Convolutions}{19}{subsubsection.8.1.1}\protected@file@percent }
\newlabel{section_number_of_convolutions_experiment}{{8.1.1}{19}{Number of Convolutions}{subsubsection.8.1.1}{}}
\newlabel{table_encoder_num_conv}{{\caption@xref {table_encoder_num_conv}{ on input line 41}}{19}{Number of Convolutions}{table.caption.12}{}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces The layers of the encoder of test $4$ up to the vector that the two dense layers use to produce the means and standard deviations for the latent code. Test $x$ only contains the layers up to the layer with $x$ in the \textit  {Test} column and the layers with \textit  {all}. The output of the \textit  {Flatten} layer varies depending on the test.\relax }}{19}{table.caption.12}\protected@file@percent }
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces The layers of the decoder of test $4$. Test $x$ only contains the layers after the layer with the number $x$ in the \textit  {Test} column and the layers with \textit  {all}. The outputs of the \textit  {Dense} and \textit  {Reshape} layers vary depending on the test.\relax }}{19}{table.caption.13}\protected@file@percent }
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces There is a graph for each test architecture. Each graph depicts the mean MAEs between input and reconstructions across all iterations in an epoch of training.\relax }}{20}{figure.caption.14}\protected@file@percent }
\newlabel{figure_learning_curves_1}{{8.1.1}{20}{Number of Convolutions}{figure.caption.14}{}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces For each test architecture the table shows the mean of the MAEs across all iterations of the last epoch, the mean loss across all iterations of the last epoch and the time it took to train the model on the personal computer as specified in section \ref  {hardware}. Notice that the loss is not the same as MAE since it additionally takes the Kullback-Leibler divergence into account.\relax }}{20}{table.caption.15}\protected@file@percent }
\newlabel{table_mae_1}{{8.1.1}{20}{Number of Convolutions}{table.caption.15}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces The left column shows original images taken from a validation set that the VAE has not seen during training. The other columns show the reconstructions of the VAE test architectures.\relax }}{20}{figure.caption.16}\protected@file@percent }
\newlabel{figure_reconstructions_1}{{8.1.1}{21}{Number of Convolutions}{figure.caption.16}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.1.2}Number of Filters}{21}{subsubsection.8.1.2}\protected@file@percent }
\newlabel{section_number_of_filters}{{8.1.2}{21}{Number of Filters}{subsubsection.8.1.2}{}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces The layers of the encoder of test $4$ up to the vector that the two dense layers use to produce the means and standard deviations for the latent code. Test $x$ only contains the layers up to the layer with $x$ in the \textit  {Test} column and the layers with \textit  {all}. The output of the \textit  {Flatten} layer varies depending on the test.\relax }}{21}{table.caption.17}\protected@file@percent }
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces The layers of the decoder of test $4$. Test $x$ only contains the layers after the layer with the number $x$ in the \textit  {Test} column and the layers with \textit  {all}. The outputs of the \textit  {Dense} and \textit  {Reshape} layers vary depending on the test.\relax }}{22}{table.caption.18}\protected@file@percent }
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces There is a graph for each test architecture. Each graph depicts the mean MAEs between input and reconstructions across all iterations in an epoch of training.\relax }}{22}{figure.caption.19}\protected@file@percent }
\newlabel{figure_learning_curves2}{{8.1.2}{22}{Number of Filters}{figure.caption.19}{}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces For each test architecture the table shows the mean of the MAEs across all iterations of the last epoch, the mean loss across all iterations of the last epoch and the time it took to train the model on the personal computer as described in section \ref  {hardware}. Notice that the loss is not the same as MAE since it additionally takes the Kullback-Leibler divergence into account.\relax }}{22}{table.caption.20}\protected@file@percent }
\newlabel{table_maes2}{{8.1.2}{22}{Number of Filters}{table.caption.20}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces The left column shows original images taken from a validation set that the VAE has not seen during training. The other columns show the reconstructions of the VAE test architectures.\relax }}{23}{figure.caption.21}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.1.3}Kernel Size}{23}{subsubsection.8.1.3}\protected@file@percent }
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces There is a graph for each test architecture. Each graph depicts the mean MAEs between input and reconstructions across all iterations in an epoch of training.\relax }}{23}{figure.caption.22}\protected@file@percent }
\newlabel{figure_learning_curves3}{{8.1.3}{24}{Kernel Size}{figure.caption.22}{}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces For each test architecture the table shows the mean of the MAEs across all iterations of the last epoch, the mean loss across all iterations of the last epoch and the time it took to train the model on the personal computer as described in section \ref  {hardware}. Notice that the loss is not the same as MAE since it additionally takes the Kullback-Leibler divergence into account.\relax }}{24}{table.caption.23}\protected@file@percent }
\newlabel{table_maes3}{{8.1.3}{24}{Kernel Size}{table.caption.23}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces The left column shows original images taken from a validation set that the VAE has not seen during training. The other columns show the reconstructions of the VAE test architectures.\relax }}{24}{figure.caption.24}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.1.4}Pooling}{24}{subsubsection.8.1.4}\protected@file@percent }
\newlabel{table_encoder_pooling}{{\caption@xref {table_encoder_pooling}{ on input line 452}}{25}{Pooling}{table.caption.25}{}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {8}{\ignorespaces The layers of the encoder of test $1$ up to the vector that the two dense layers use to produce the means and standard deviations for the latent code.\relax }}{25}{table.caption.25}\protected@file@percent }
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {9}{\ignorespaces The layers of the decoder of test $1$.\relax }}{25}{table.caption.26}\protected@file@percent }
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces There is a graph for each test architecture. Each graph depicts the mean MAEs between input and reconstructions across all iterations in an epoch of training.\relax }}{25}{figure.caption.27}\protected@file@percent }
\newlabel{figure_learning_curves4}{{8.1.4}{25}{Pooling}{figure.caption.27}{}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {10}{\ignorespaces For each test architecture the table shows the mean of the MAEs across all iterations of the last epoch, the mean loss across all iterations of the last epoch and the time it took to train the model on the personal computer as described in section \ref  {hardware}. Notice that the loss is not the same as MAE since it additionally takes the Kullback-Leibler divergence into account.\relax }}{26}{table.caption.28}\protected@file@percent }
\newlabel{table_maes4}{{8.1.4}{26}{Pooling}{table.caption.28}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces The left column shows original images taken from a validation set that the VAE has not seen during training. The other columns show the reconstructions of the VAE test architectures.\relax }}{26}{figure.caption.29}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.1.5}Best Architectures}{26}{subsubsection.8.1.5}\protected@file@percent }
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces The left column shows original images taken from a validation set that the VAE has not seen during training. Column $(a)$ shows the reconstructions of the fully convolutional VAE while column $(b)$ depicts the reconstructions of the max pooling architecture.\relax }}{27}{figure.caption.30}\protected@file@percent }
\newlabel{figure_results_trained_on_all}{{8.1.5}{27}{Best Architectures}{figure.caption.30}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {8.2}Latent Space}{27}{subsection.8.2}\protected@file@percent }
\newlabel{latent_space_experiments}{{8.2}{27}{Latent Space}{subsection.8.2}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.2.1}Fully convolutional Architecture}{28}{subsubsection.8.2.1}\protected@file@percent }
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces PCA results in the top row. T-SNE results in the bottom row. The coloring corresponds to the predominant classes in the input images with green for vegetation, grey for ground, blue for water, red for buildings and yellow for clutter.\relax }}{28}{figure.caption.31}\protected@file@percent }
\newlabel{figure_classes_convolutional}{{8.2.1}{28}{Fully convolutional Architecture}{figure.caption.31}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces PCA results in the top row. T-SNE results in the bottom row. The coloring corresponds to the average of the height values in the digital surface model for the input image. Darker points represent a higher average height.\relax }}{29}{figure.caption.32}\protected@file@percent }
\newlabel{figure_heights_convolutional}{{8.2.1}{29}{Fully convolutional Architecture}{figure.caption.32}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces Two dimensional output points of t-SNE of $4000$ latent codes. The points are represented by the input images that produced the latent code corresponding to the two dimensional representation.\relax }}{30}{figure.caption.33}\protected@file@percent }
\newlabel{figure_images_convolutional_tsne}{{19}{30}{Two dimensional output points of t-SNE of $4000$ latent codes. The points are represented by the input images that produced the latent code corresponding to the two dimensional representation.\relax }{figure.caption.33}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces The left column shows original images taken from a validation set that the VAE has not seen during training. The other columns show the reconstructions produced by the VAEs with the different coding sizes.\relax }}{31}{figure.caption.34}\protected@file@percent }
\newlabel{figure_reconstructions_convolutional}{{20}{31}{The left column shows original images taken from a validation set that the VAE has not seen during training. The other columns show the reconstructions produced by the VAEs with the different coding sizes.\relax }{figure.caption.34}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {21}{\ignorespaces Clusters of VAE with coding size $50$\relax }}{31}{figure.caption.35}\protected@file@percent }
\newlabel{figure_closer_clusters}{{21}{31}{Clusters of VAE with coding size $50$\relax }{figure.caption.35}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.2.2}Pooling Architecture}{32}{subsubsection.8.2.2}\protected@file@percent }
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {22}{\ignorespaces  PCA results in the top row. T-SNE results in the bottom row. The coloring represents the predominant class in the input with green for vegetation, grey for ground, blue for water, red for buildings and yellow for clutter.\relax }}{32}{figure.caption.36}\protected@file@percent }
\newlabel{figure_classes_pooling}{{8.2.2}{32}{Pooling Architecture}{figure.caption.36}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {23}{\ignorespaces PCA results in the top row. T-SNE results in the bottom row. The coloring corresponds to the average of the height values in the digital surface model for the input image. Darker points represent a higher average height.\relax }}{32}{figure.caption.37}\protected@file@percent }
\newlabel{figure_heights_pooling}{{8.2.2}{32}{Pooling Architecture}{figure.caption.37}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {24}{\ignorespaces Two dimensional output points of t-SNE of $4000$ latent codes. The points are represented by the input images that produced the latent code corresponding to the two dimensional representation.\relax }}{33}{figure.caption.38}\protected@file@percent }
\newlabel{figure_images_pooling_PCA}{{8.2.2}{33}{Pooling Architecture}{figure.caption.38}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {9}Conclusion and Future Work}{33}{section.9}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {9.1}Conclusion}{33}{subsection.9.1}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {9.2}Future Work}{34}{subsection.9.2}\protected@file@percent }
\abx@aux@page{37}{35}
\abx@aux@page{38}{35}
\abx@aux@page{39}{35}
\abx@aux@page{40}{35}
\abx@aux@page{41}{35}
\abx@aux@page{42}{35}
\abx@aux@page{43}{35}
\abx@aux@page{44}{35}
\abx@aux@page{45}{35}
\abx@aux@page{46}{35}
\abx@aux@page{47}{35}
\abx@aux@page{48}{35}
\abx@aux@page{49}{35}
\abx@aux@page{50}{35}
\abx@aux@page{51}{35}
\abx@aux@page{52}{35}
\abx@aux@page{53}{35}
\abx@aux@page{54}{35}
\abx@aux@page{55}{35}
\abx@aux@refcontextdefaultsdone
\abx@aux@defaultrefcontext{0}{2019-bosch-semantic}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{2016-doersch-tutorial}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{2017-geron-homl}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{2016-goodfellow-deep}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{2016-jegou-the}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{2018-kendall-multi}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{2017-klambauer-selu}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{2012-krizhevsky-imagenet}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{2008-vanDerMaaten-visualizing}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{2015-martin-tensorflow}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{2016-mishkin-systematic}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{2018-montavon-methods}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{2018-Pedamonti-comparison}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{2011-pedregosa-scikit}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{1995-rossum-python}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{2019-schmitz-semantic}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{2017-shelhamer-fully}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{2015-springenberg-striving}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{2015-theis-generative}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{2019-vandenhende-branched}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{2018-zhang-visual}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{2015-Chervinskii-autoencoder}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{2005-chrislb-artificial-neuron}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{2013-glosser-ann}{nyt/global//global/global}
\abx@aux@page{56}{36}
\abx@aux@page{57}{36}
\abx@aux@page{58}{36}
\abx@aux@page{59}{36}
\abx@aux@page{60}{36}
