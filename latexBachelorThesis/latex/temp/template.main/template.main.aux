\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\abx@aux@refcontext{nyt/global//global/global}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\@writefile{lof}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\@writefile{lot}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\abx@aux@cite{2017-shelhamer-fully}
\abx@aux@segm{0}{0}{2017-shelhamer-fully}
\abx@aux@cite{2016-jegou-the}
\abx@aux@segm{0}{0}{2016-jegou-the}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {1}Abstract}{2}{section.1}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {2}Introduction}{2}{section.2}\protected@file@percent }
\abx@aux@page{1}{2}
\abx@aux@cite{2018-kendall-multi}
\abx@aux@segm{0}{0}{2018-kendall-multi}
\abx@aux@cite{2019-schmitz-semantic}
\abx@aux@segm{0}{0}{2019-schmitz-semantic}
\abx@aux@segm{0}{0}{2019-schmitz-semantic}
\abx@aux@cite{2019-vandenhende-branched}
\abx@aux@segm{0}{0}{2019-vandenhende-branched}
\abx@aux@page{2}{3}
\abx@aux@page{3}{3}
\abx@aux@page{4}{3}
\abx@aux@page{5}{3}
\abx@aux@page{6}{3}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {3}Background}{3}{section.3}\protected@file@percent }
\abx@aux@cite{2005-chrislb-artificial-neuron}
\abx@aux@segm{0}{0}{2005-chrislb-artificial-neuron}
\abx@aux@segm{0}{0}{2005-chrislb-artificial-neuron}
\abx@aux@cite{2017-geron-homl}
\abx@aux@segm{0}{0}{2017-geron-homl}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Remote Sensing}{4}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Artificial Neuron}{4}{subsection.3.2}\protected@file@percent }
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces An artificial neuron. Image taken from \parencite {2005-chrislb-artificial-neuron}\relax }}{4}{figure.caption.2}\protected@file@percent }
\abx@aux@cite{2013-glosser-ann}
\abx@aux@segm{0}{0}{2013-glosser-ann}
\abx@aux@segm{0}{0}{2013-glosser-ann}
\abx@aux@cite{2016-goodfellow-deep}
\abx@aux@segm{0}{0}{2016-goodfellow-deep}
\abx@aux@segm{0}{0}{2017-geron-homl}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Artificial Neural Network}{5}{subsection.3.3}\protected@file@percent }
\abx@aux@page{9}{5}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces An artificial neural network with three fully connected layers. Image taken from \parencite {2013-glosser-ann}\relax }}{5}{figure.caption.3}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{figure_fully_connected_nn}{{2}{5}{An artificial neural network with three fully connected layers. Image taken from \parencite {2013-glosser-ann}\relax }{figure.caption.3}{}}
\abx@aux@segm{0}{0}{2017-geron-homl}
\abx@aux@segm{0}{0}{2017-geron-homl}
\abx@aux@segm{0}{0}{2016-goodfellow-deep}
\abx@aux@cite{2012-krizhevsky-imagenet}
\abx@aux@segm{0}{0}{2012-krizhevsky-imagenet}
\abx@aux@page{12}{6}
\abx@aux@page{13}{6}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Convolutional Neural Networks}{6}{subsection.3.4}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Pooling}{6}{section*.5}\protected@file@percent }
\abx@aux@page{16}{6}
\abx@aux@cite{2015-Chervinskii-autoencoder}
\abx@aux@segm{0}{0}{2015-Chervinskii-autoencoder}
\abx@aux@segm{0}{0}{2015-Chervinskii-autoencoder}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces A convolutional network with 12 filters in the first convolutional layer and 7 filters in the second convolutional layer. Each filters can learn different features and an RGB image is depicted as the input here. Image taken from \parencite {2017-geron-homl}\relax }}{7}{figure.caption.4}\protected@file@percent }
\newlabel{figure_cnn_filter}{{3}{7}{A convolutional network with 12 filters in the first convolutional layer and 7 filters in the second convolutional layer. Each filters can learn different features and an RGB image is depicted as the input here. Image taken from \parencite {2017-geron-homl}\relax }{figure.caption.4}{}}
\abx@aux@page{17}{7}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Autoencoders}{7}{subsection.3.5}\protected@file@percent }
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces An undercomplete autoencoder has a latent code $z$ whose dimension is smaller than the dimension of the input $x$. Image taken from \parencite {2015-Chervinskii-autoencoder}\relax }}{8}{figure.caption.6}\protected@file@percent }
\newlabel{figure_undercomplete_ae}{{4}{8}{An undercomplete autoencoder has a latent code $z$ whose dimension is smaller than the dimension of the input $x$. Image taken from \parencite {2015-Chervinskii-autoencoder}\relax }{figure.caption.6}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.6}Variational Autoencoders}{8}{subsection.3.6}\protected@file@percent }
\newlabel{vae_background}{{3.6}{8}{Variational Autoencoders}{subsection.3.6}{}}
\abx@aux@cite{2016-doersch-tutorial}
\abx@aux@segm{0}{0}{2016-doersch-tutorial}
\abx@aux@page{20}{9}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.7}Kullback-Leibler Divergence}{9}{subsection.3.7}\protected@file@percent }
\newlabel{KL-Divergence}{{3.7}{9}{Kullback-Leibler Divergence}{subsection.3.7}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.7.1}Entropy}{9}{subsubsection.3.7.1}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.7.2}Cross Entropy}{10}{subsubsection.3.7.2}\protected@file@percent }
\abx@aux@cite{2008-vanDerMaaten-visualizing}
\abx@aux@segm{0}{0}{2008-vanDerMaaten-visualizing}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.7.3}Kullback-Leibler Divergence}{11}{subsubsection.3.7.3}\protected@file@percent }
\newlabel{eq1}{{3}{11}{Kullback-Leibler Divergence}{equation.3.3}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.8}t-Distributed Stochastic Neighbor Embedding}{11}{subsection.3.8}\protected@file@percent }
\newlabel{t-sne}{{3.8}{11}{t-Distributed Stochastic Neighbor Embedding}{subsection.3.8}{}}
\abx@aux@page{21}{11}
\newlabel{pij}{{4}{11}{t-Distributed Stochastic Neighbor Embedding}{equation.3.4}{}}
\newlabel{qij}{{5}{11}{t-Distributed Stochastic Neighbor Embedding}{equation.3.5}{}}
\abx@aux@cite{1995-rossum-python}
\abx@aux@segm{0}{0}{1995-rossum-python}
\abx@aux@cite{2015-martin-tensorflow}
\abx@aux@segm{0}{0}{2015-martin-tensorflow}
\abx@aux@cite{2011-pedregosa-scikit}
\abx@aux@segm{0}{0}{2011-pedregosa-scikit}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {4}Methodology}{12}{section.4}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Environment}{12}{subsection.4.1}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.1}Hardware}{12}{subsubsection.4.1.1}\protected@file@percent }
\newlabel{hardware}{{4.1.1}{12}{Hardware}{subsubsection.4.1.1}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Personal Computer}{12}{section*.7}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Remote Machine}{12}{section*.8}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.2}Software}{12}{subsubsection.4.1.2}\protected@file@percent }
\abx@aux@page{22}{12}
\abx@aux@page{23}{12}
\abx@aux@cite{2019-bosch-semantic}
\abx@aux@segm{0}{0}{2019-bosch-semantic}
\abx@aux@page{24}{13}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Datasets}{13}{subsection.4.2}\protected@file@percent }
\newlabel{datasets}{{4.2}{13}{Datasets}{subsection.4.2}{}}
\abx@aux@page{25}{13}
\abx@aux@segm{0}{0}{2017-geron-homl}
\abx@aux@cite{2017-klambauer-selu}
\abx@aux@segm{0}{0}{2017-klambauer-selu}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Architecture}{14}{subsection.4.3}\protected@file@percent }
\newlabel{architecture}{{4.3}{14}{Architecture}{subsection.4.3}{}}
\abx@aux@page{26}{14}
\abx@aux@segm{0}{0}{2017-klambauer-selu}
\abx@aux@cite{2018-Pedamonti-comparison}
\abx@aux@segm{0}{0}{2018-Pedamonti-comparison}
\abx@aux@cite{2015-springenberg-striving}
\abx@aux@segm{0}{0}{2015-springenberg-striving}
\abx@aux@page{27}{15}
\abx@aux@page{28}{15}
\abx@aux@page{29}{15}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.1}Fully Convolutional Architecture}{15}{subsubsection.4.3.1}\protected@file@percent }
\newlabel{section_pure_convolutional_architecture}{{4.3.1}{15}{Fully Convolutional Architecture}{subsubsection.4.3.1}{}}
\abx@aux@page{30}{15}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.2}Max Pooling Architecture}{15}{subsubsection.4.3.2}\protected@file@percent }
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Encoder network architecture with an encoding size of 1024. The kernel size is $3\times 3$ in every layer. The reparameterization trick in the last layer refers to the sampling as described in \autoref  {vae_background}. The dimensions of each layers output are specified with depth first, the height second and the width third.\relax }}{16}{figure.caption.9}\protected@file@percent }
\newlabel{figure_pure_convolutional_encoder}{{5}{16}{Encoder network architecture with an encoding size of 1024. The kernel size is $3\times 3$ in every layer. The reparameterization trick in the last layer refers to the sampling as described in \autoref {vae_background}. The dimensions of each layers output are specified with depth first, the height second and the width third.\relax }{figure.caption.9}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Decoder network architecture with an encoding size of 1024. The kernel size is $3\times 3$ in every layer. The last deconvolutional layer has a sigmoid activation function. The dimensions of each layers output are specified with depth first, the height second and the width third.\relax }}{16}{figure.caption.10}\protected@file@percent }
\newlabel{figure_decoder}{{6}{16}{Decoder network architecture with an encoding size of 1024. The kernel size is $3\times 3$ in every layer. The last deconvolutional layer has a sigmoid activation function. The dimensions of each layers output are specified with depth first, the height second and the width third.\relax }{figure.caption.10}{}}
\abx@aux@segm{0}{0}{2008-vanDerMaaten-visualizing}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Part of the encoder network architecture with pooling and an encoding size of 1024. The convolutional kernel always has size $3\times 3$. The pooling window always has size $2\times 2$. The rest of the encoder that generates the latent code is the same as in the fully convolutional encoder in \autoref  {section_pure_convolutional_architecture}. The dimensions of each layers output are specified with depth first, the height second and the width third.\relax }}{17}{figure.caption.11}\protected@file@percent }
\newlabel{figure_encoder_pooling}{{7}{17}{Part of the encoder network architecture with pooling and an encoding size of 1024. The convolutional kernel always has size $3\times 3$. The pooling window always has size $2\times 2$. The rest of the encoder that generates the latent code is the same as in the fully convolutional encoder in \autoref {section_pure_convolutional_architecture}. The dimensions of each layers output are specified with depth first, the height second and the width third.\relax }{figure.caption.11}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Understanding Latent Space}{17}{subsection.4.4}\protected@file@percent }
\newlabel{section_understanding_latent_space}{{4.4}{17}{Understanding Latent Space}{subsection.4.4}{}}
\abx@aux@page{31}{17}
\abx@aux@cite{2016-mishkin-systematic}
\abx@aux@segm{0}{0}{2016-mishkin-systematic}
\abx@aux@cite{2015-theis-generative}
\abx@aux@segm{0}{0}{2015-theis-generative}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {5}Experiments}{18}{section.5}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Variational Autoencoder Architecture}{18}{subsection.5.1}\protected@file@percent }
\newlabel{architecture_experiments}{{5.1}{18}{Variational Autoencoder Architecture}{subsection.5.1}{}}
\abx@aux@page{32}{18}
\abx@aux@page{33}{18}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.1}Number of Convolutions}{19}{subsubsection.5.1.1}\protected@file@percent }
\newlabel{section_number_of_convolutions_experiment}{{5.1.1}{19}{Number of Convolutions}{subsubsection.5.1.1}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Experiment Architectures}{19}{section*.12}\protected@file@percent }
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces The layers of the encoder of test $4$ up to the vector that the two dense layers use to produce the means and standard deviations for the latent code. Test $x$ only contains the layers up to the layer with $x$ in the \textit  {Test} column and the layers with \textit  {all}. The output of the \textit  {Flatten} layer varies depending on the test.\relax }}{19}{table.caption.13}\protected@file@percent }
\newlabel{table_encoder_num_conv}{{1}{19}{The layers of the encoder of test $4$ up to the vector that the two dense layers use to produce the means and standard deviations for the latent code. Test $x$ only contains the layers up to the layer with $x$ in the \textit {Test} column and the layers with \textit {all}. The output of the \textit {Flatten} layer varies depending on the test.\relax }{table.caption.13}{}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces The layers of the decoder of test $4$. Test $x$ only contains the layers after the layer with the number $x$ in the \textit  {Test} column and the layers with \textit  {all}. The outputs of the \textit  {Dense} and \textit  {Reshape} layers vary depending on the test.\relax }}{19}{table.caption.14}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Results}{20}{section*.15}\protected@file@percent }
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces There is a graph for each test architecture. Each graph depicts the mean MAEs between input and reconstructions across all iterations in an epoch of training.\relax }}{20}{figure.caption.16}\protected@file@percent }
\newlabel{figure_learning_curves_1}{{8}{20}{There is a graph for each test architecture. Each graph depicts the mean MAEs between input and reconstructions across all iterations in an epoch of training.\relax }{figure.caption.16}{}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces For each test architecture the table shows the mean of the MAEs across all iterations of the last epoch, the mean loss across all iterations of the last epoch and the time it took to train the model on the personal computer as specified in \autoref  {hardware}. Note that the loss is not the same as MAE since it additionally takes the Kullback-Leibler divergence into account.\relax }}{20}{table.caption.17}\protected@file@percent }
\newlabel{table_mae_1}{{3}{20}{For each test architecture the table shows the mean of the MAEs across all iterations of the last epoch, the mean loss across all iterations of the last epoch and the time it took to train the model on the personal computer as specified in \autoref {hardware}. Note that the loss is not the same as MAE since it additionally takes the Kullback-Leibler divergence into account.\relax }{table.caption.17}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces The left column shows original images taken from a validation set that the VAE has not seen during training. The other columns show the reconstructions of the VAE test architectures.\relax }}{21}{figure.caption.18}\protected@file@percent }
\newlabel{figure_reconstructions_1}{{9}{21}{The left column shows original images taken from a validation set that the VAE has not seen during training. The other columns show the reconstructions of the VAE test architectures.\relax }{figure.caption.18}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Discussion}{21}{section*.19}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.2}Number of Filters}{21}{subsubsection.5.1.2}\protected@file@percent }
\newlabel{section_number_of_filters}{{5.1.2}{21}{Number of Filters}{subsubsection.5.1.2}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Experiment Architectures}{22}{section*.20}\protected@file@percent }
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces The layers of the encoder of test $4$ up to the vector that the two dense layers use to produce the means and standard deviations for the latent code. Test $x$ only contains the layers up to the layer with $x$ in the \textit  {Test} column and the layers with \textit  {all}. The output of the \textit  {Flatten} layer varies depending on the test.\relax }}{22}{table.caption.21}\protected@file@percent }
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces The layers of the decoder of test $4$. Test $x$ only contains the layers after the layer with the number $x$ in the \textit  {Test} column and the layers with \textit  {all}. The outputs of the \textit  {Dense} and \textit  {Reshape} layers vary depending on the test.\relax }}{22}{table.caption.22}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Results}{22}{section*.23}\protected@file@percent }
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces There is a graph for each test architecture. Each graph depicts the mean MAEs between input and reconstructions across all iterations in an epoch of training.\relax }}{22}{figure.caption.24}\protected@file@percent }
\newlabel{figure_learning_curves2}{{10}{22}{There is a graph for each test architecture. Each graph depicts the mean MAEs between input and reconstructions across all iterations in an epoch of training.\relax }{figure.caption.24}{}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces For each test architecture the table shows the mean of the MAEs across all iterations of the last epoch, the mean loss across all iterations of the last epoch and the time it took to train the model on the personal computer as described in \autoref  {hardware}. Note that the loss is not the same as MAE since it additionally takes the Kullback-Leibler divergence into account.\relax }}{23}{table.caption.25}\protected@file@percent }
\newlabel{table_maes2}{{6}{23}{For each test architecture the table shows the mean of the MAEs across all iterations of the last epoch, the mean loss across all iterations of the last epoch and the time it took to train the model on the personal computer as described in \autoref {hardware}. Note that the loss is not the same as MAE since it additionally takes the Kullback-Leibler divergence into account.\relax }{table.caption.25}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces The left column shows original images taken from a validation set that the VAE has not seen during training. The other columns show the reconstructions of the VAE test architectures.\relax }}{23}{figure.caption.26}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Discussion}{23}{section*.27}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.3}Kernel Size}{23}{subsubsection.5.1.3}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Results}{24}{section*.28}\protected@file@percent }
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces There is a graph for each test architecture. Each graph depicts the mean MAEs between input and reconstructions across all iterations in an epoch of training.\relax }}{24}{figure.caption.29}\protected@file@percent }
\newlabel{figure_learning_curves3}{{12}{24}{There is a graph for each test architecture. Each graph depicts the mean MAEs between input and reconstructions across all iterations in an epoch of training.\relax }{figure.caption.29}{}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces For each test architecture the table shows the mean of the MAEs across all iterations of the last epoch, the mean loss across all iterations of the last epoch and the time it took to train the model on the personal computer as described in \autoref  {hardware}. Note that the loss is not the same as MAE since it additionally takes the Kullback-Leibler divergence into account.\relax }}{24}{table.caption.30}\protected@file@percent }
\newlabel{table_maes3}{{7}{24}{For each test architecture the table shows the mean of the MAEs across all iterations of the last epoch, the mean loss across all iterations of the last epoch and the time it took to train the model on the personal computer as described in \autoref {hardware}. Note that the loss is not the same as MAE since it additionally takes the Kullback-Leibler divergence into account.\relax }{table.caption.30}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces The left column shows original images taken from a validation set that the VAE has not seen during training. The other columns show the reconstructions of the VAE test architectures.\relax }}{25}{figure.caption.31}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Discussion}{25}{section*.32}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.4}Pooling}{25}{subsubsection.5.1.4}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Experiment Architectures}{25}{section*.33}\protected@file@percent }
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {8}{\ignorespaces The layers of the encoder of test $1$ up to the vector that the two dense layers use to produce the means and standard deviations for the latent code.\relax }}{26}{table.caption.34}\protected@file@percent }
\newlabel{table_encoder_pooling}{{8}{26}{The layers of the encoder of test $1$ up to the vector that the two dense layers use to produce the means and standard deviations for the latent code.\relax }{table.caption.34}{}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {9}{\ignorespaces The layers of the decoder of test $1$.\relax }}{26}{table.caption.35}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Results}{26}{section*.36}\protected@file@percent }
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces There is a graph for each test architecture. Each graph depicts the mean MAEs between input and reconstructions across all iterations in an epoch of training.\relax }}{26}{figure.caption.37}\protected@file@percent }
\newlabel{figure_learning_curves4}{{14}{26}{There is a graph for each test architecture. Each graph depicts the mean MAEs between input and reconstructions across all iterations in an epoch of training.\relax }{figure.caption.37}{}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {10}{\ignorespaces For each test architecture the table shows the mean of the MAEs across all iterations of the last epoch, the mean loss across all iterations of the last epoch and the time it took to train the model on the personal computer as described in \autoref  {hardware}. Note that the loss is not the same as MAE since it additionally takes the Kullback-Leibler divergence into account.\relax }}{27}{table.caption.38}\protected@file@percent }
\newlabel{table_maes4}{{10}{27}{For each test architecture the table shows the mean of the MAEs across all iterations of the last epoch, the mean loss across all iterations of the last epoch and the time it took to train the model on the personal computer as described in \autoref {hardware}. Note that the loss is not the same as MAE since it additionally takes the Kullback-Leibler divergence into account.\relax }{table.caption.38}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces The left column shows original images taken from a validation set that the VAE has not seen during training. The other columns show the reconstructions of the VAE test architectures.\relax }}{27}{figure.caption.39}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {paragraph}{Discussion}{27}{section*.40}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.5}Best Architectures}{27}{subsubsection.5.1.5}\protected@file@percent }
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces The left column shows original images taken from a validation set that the VAE has not seen during training. Column $(a)$ shows the reconstructions of the fully convolutional VAE while column $(b)$ depicts the reconstructions of the max pooling architecture.\relax }}{28}{figure.caption.41}\protected@file@percent }
\newlabel{figure_results_trained_on_all}{{16}{28}{The left column shows original images taken from a validation set that the VAE has not seen during training. Column $(a)$ shows the reconstructions of the fully convolutional VAE while column $(b)$ depicts the reconstructions of the max pooling architecture.\relax }{figure.caption.41}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Latent Space}{28}{subsection.5.2}\protected@file@percent }
\newlabel{latent_space_experiments}{{5.2}{28}{Latent Space}{subsection.5.2}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.1}Fully convolutional Architecture}{29}{subsubsection.5.2.1}\protected@file@percent }
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces PCA results in the top row. T-SNE results in the bottom row. The coloring corresponds to the predominant classes in the input images with green for vegetation, grey for ground, blue for water, red for buildings and yellow for clutter.\relax }}{29}{figure.caption.42}\protected@file@percent }
\newlabel{figure_classes_convolutional}{{17}{29}{PCA results in the top row. T-SNE results in the bottom row. The coloring corresponds to the predominant classes in the input images with green for vegetation, grey for ground, blue for water, red for buildings and yellow for clutter.\relax }{figure.caption.42}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces PCA results in the top row. T-SNE results in the bottom row. The coloring corresponds to the average of the height values in the digital surface model for the input image. Darker points represent a higher average height.\relax }}{30}{figure.caption.43}\protected@file@percent }
\newlabel{figure_heights_convolutional}{{18}{30}{PCA results in the top row. T-SNE results in the bottom row. The coloring corresponds to the average of the height values in the digital surface model for the input image. Darker points represent a higher average height.\relax }{figure.caption.43}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces Two dimensional output points of t-SNE of $4000$ latent codes. The points are represented by the input images that produced the latent code corresponding to the two dimensional representation.\relax }}{31}{figure.caption.44}\protected@file@percent }
\newlabel{figure_images_convolutional_tsne}{{19}{31}{Two dimensional output points of t-SNE of $4000$ latent codes. The points are represented by the input images that produced the latent code corresponding to the two dimensional representation.\relax }{figure.caption.44}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces The left column shows original images taken from a validation set that the VAE has not seen during training. The other columns show the reconstructions produced by the VAEs with the different coding sizes.\relax }}{32}{figure.caption.45}\protected@file@percent }
\newlabel{figure_reconstructions_convolutional}{{20}{32}{The left column shows original images taken from a validation set that the VAE has not seen during training. The other columns show the reconstructions produced by the VAEs with the different coding sizes.\relax }{figure.caption.45}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {21}{\ignorespaces Clusters of VAE with coding size $50$\relax }}{32}{figure.caption.46}\protected@file@percent }
\newlabel{figure_closer_clusters}{{21}{32}{Clusters of VAE with coding size $50$\relax }{figure.caption.46}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.2}Pooling Architecture}{33}{subsubsection.5.2.2}\protected@file@percent }
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {22}{\ignorespaces  PCA results in the top row. T-SNE results in the bottom row. The coloring represents the predominant class in the input with green for vegetation, grey for ground, blue for water, red for buildings and yellow for clutter.\relax }}{33}{figure.caption.47}\protected@file@percent }
\newlabel{figure_classes_pooling}{{22}{33}{PCA results in the top row. T-SNE results in the bottom row. The coloring represents the predominant class in the input with green for vegetation, grey for ground, blue for water, red for buildings and yellow for clutter.\relax }{figure.caption.47}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {23}{\ignorespaces PCA results in the top row. T-SNE results in the bottom row. The coloring corresponds to the average of the height values in the digital surface model for the input image. Darker points represent a higher average height.\relax }}{33}{figure.caption.48}\protected@file@percent }
\newlabel{figure_heights_pooling}{{23}{33}{PCA results in the top row. T-SNE results in the bottom row. The coloring corresponds to the average of the height values in the digital surface model for the input image. Darker points represent a higher average height.\relax }{figure.caption.48}{}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {24}{\ignorespaces Two dimensional output points of t-SNE of $4000$ latent codes. The points are represented by the input images that produced the latent code corresponding to the two dimensional representation.\relax }}{34}{figure.caption.49}\protected@file@percent }
\newlabel{figure_images_pooling_PCA}{{24}{34}{Two dimensional output points of t-SNE of $4000$ latent codes. The points are represented by the input images that produced the latent code corresponding to the two dimensional representation.\relax }{figure.caption.49}{}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusion and Future Work}{34}{section.6}\protected@file@percent }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Conclusion}{34}{subsection.6.1}\protected@file@percent }
\abx@aux@cite{2015-bach-on}
\abx@aux@segm{0}{0}{2015-bach-on}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Future Work}{35}{subsection.6.2}\protected@file@percent }
\abx@aux@page{34}{35}
\abx@aux@page{35}{36}
\abx@aux@page{36}{36}
\abx@aux@page{37}{36}
\abx@aux@page{38}{36}
\abx@aux@page{39}{36}
\abx@aux@page{40}{36}
\abx@aux@page{41}{36}
\abx@aux@page{42}{36}
\abx@aux@page{43}{36}
\abx@aux@page{44}{36}
\abx@aux@page{45}{36}
\abx@aux@page{46}{36}
\abx@aux@page{47}{36}
\abx@aux@page{48}{36}
\abx@aux@page{49}{36}
\abx@aux@page{50}{36}
\abx@aux@page{51}{36}
\abx@aux@page{52}{36}
\abx@aux@page{53}{36}
\abx@aux@refcontextdefaultsdone
\abx@aux@defaultrefcontext{0}{2015-bach-on}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{2019-bosch-semantic}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{2016-doersch-tutorial}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{2017-geron-homl}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{2016-goodfellow-deep}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{2016-jegou-the}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{2018-kendall-multi}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{2017-klambauer-selu}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{2012-krizhevsky-imagenet}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{2008-vanDerMaaten-visualizing}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{2015-martin-tensorflow}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{2016-mishkin-systematic}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{2018-Pedamonti-comparison}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{2011-pedregosa-scikit}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{1995-rossum-python}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{2019-schmitz-semantic}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{2017-shelhamer-fully}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{2015-springenberg-striving}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{2015-theis-generative}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{2019-vandenhende-branched}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{2015-Chervinskii-autoencoder}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{2005-chrislb-artificial-neuron}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{2013-glosser-ann}{nyt/global//global/global}
\abx@aux@page{54}{37}
\abx@aux@page{55}{37}
\abx@aux@page{56}{37}
\abx@aux@page{57}{37}
