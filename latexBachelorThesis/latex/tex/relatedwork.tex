\section{Related Work}

This section presents concrete insights from related literature that influenced the used methodology. It is 
divided into two subsections. The first one, being convolutional network architecture, covers common practice about
hyper parameters and other choices for convolutional models while the second subsection focuses on generative aspects
and autoencoders.

\subsection{Convolutional Network Architecture}

In their paper \textit{Striving for simplicity: The All Convolutional Net} \parencite{2015-springenberg-striving}
the authors find that including pooling layers such as max pooling into a convolutional architecture does not 
necessarily mean an improvement. When using a large enough network the invariances that should be
introduced by pooling layers can also be learned in a network with convolutional layers only. Thus, in certain cases,
convolutional layers with strides of two can be used to reduce dimensionality in a CNN instead of pooling layers
which is why both options are explored in this work.\\

The paper \textit{Systematic evaluation of CNN advances on the ImageNet} \parencite{2016-mishkin-systematic} analyzes
the impact of multiple learning parameter and architecture choices on the performance of convolutional neural networks.
One conclusion it reaches is the suggestion to use batch sizes around 128 or 256. 