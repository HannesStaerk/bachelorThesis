\section{Related Work} \label{related_work_general_architecture}

This section presents concrete insights from related literature that influenced the used methodology.
It covers common practice about
hyper parameters and other choices for convolutional models as well as information about 
generative aspects models and autoencoders.\\

\textcite{2015-springenberg-striving}
find that including pooling layers such as max pooling into a convolutional architecture does not 
necessarily mean an improvement. When using a large enough network the invariances that should be
introduced by pooling layers can also be learned in a network with convolutional layers only. Thus, in certain cases,
convolutional layers with strides of two can be used to reduce dimensionality in a CNN instead of pooling layers
which is why both options are explored in this work.
\\

\textcite{2016-mishkin-systematic} analyze
the impact of multiple learning parameter and architecture choices on the performance of convolutional neural networks.
One conclusion it reaches is the suggestion to use batch sizes around 128 or 256.\\

\textcite{2017-klambauer-selu} show that using
a scaled exponential linear unit (SELU) as activation function leads to self normalizing networks. These networks 
eliminate the problem of vanishing or exploding gradients. Additionally the authors prove superiority of SELU 
over other normalization techniques in their benchmarks. 

This is in line with the results of \textcite{2018-Pedamonti-comparison} reaching the conclusion that SELU leads
to faster learning rates than other activation functions like ReLU or Leaky ReLU.\\

\textcite{2015-theis-generative}
conclude that there are many difficulties in
measuring the optimization and performance of generative models. A clear conclusion is that there is no 
one fits all metric for the quality of models like variational autoencoders. While the intuitive evaluation
of visual results favors overfitted models, pure reliance on numerical measures is not appropriate when the
goal is image synthesis. Overall the quality of evaluation measure is largely dependent on the application
and goal.\\



