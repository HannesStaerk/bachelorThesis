\section{Methodology}

In this section the subsection environment will explain what tools were used to program the autoencoders
and what hardware the developed models were trained and written on. The subsection Datasets 
contains information about the remote sensing image data that was analyzed using the autoencoders. 
The architecture subsection lays out 
the design of the different models regarding the layers and loss functions. In the last subsection 
latent space there are the techniques used to visualize and understand the latent code between the 
encoder and decoder with comparisons of the latent space between normal autoencoders and variational
autoencoders.

\subsection{Environment}

\subsubsection{Hardware}

Training autoencoders with larger images of multiple channels takes a lot of computation especially if the 
features that should be learned are as complicated and abstract as the topography of remote sensing data.
As this is very time consuming the training of the models was mainly done on a remote machine from the
Leibniz University in Hannover that could run 24/7. Only the
programming of the models and tests with small datasets took place on a local personal computer that had no
GPU that was capable of training models in the given scenario. The following
are the specifications of the used systems:\\

\paragraph{Personal Computer} \mbox{} \smallskip \\

AMD Ryzen 7 1700, 16 GB RAM, NVIDIA GeForce GTX 760 2GB

\paragraph{Remote Machine} \mbox{} \smallskip \\

Lenovo Legion Y520T-25IKL, Intel i7-7700, 8GB RAM, NVIDIA GeForce GTX 1060 3GB

\subsubsection{Software}

The programming language used for the work is Python with the development environment PyCharm locally
and and Jupyter Notebooks on a remote machine. The machine learning framework Tensorflow was used which
is developed by Google and offers cross-platform support, running on most CPUs and GPUs \parencite{tensorflow}.
Whenever possible Tensorflows implementation of the Keras API specification was used which is a high level API
for training machine learning models.

\subsection{Datasets} \label{datasets}

The available data are 1024x1024 images of the two United States cities Jacksonville
in Florida and Omaha in Nebraska taken from the US3D Dataset that
was partially published to provide research data for the problem
of 3D reconstruction \parencite{2019-bosch-semantic}.
The images for each recorded area cover one square kilometer and can be divided 
into four categories with the first one being multispectral satellite images with eight channels (MSI). 
From the MSI data three channels were extracted and used as red, green and blue intensities (RGB). 
Thirdly there are digital surface models (DSM) and Lastly semantic labeling with five different categories. \\

The MSI data was collected by the WorldView-3 satellite of Digital Globe from 2014 to 2016.
Thereby the images were taken in different seasons and times of day leading to great differences
in their appearance regarding for instance shadows, reflections, overall brightness or clouds.
This is an advantage for training models that are capable of processing data with similar differences in 
appearance.
In the MSI dataset a single picture consists of eight channels for eight different bands of the spectrum with
a ground sample distance of 1.3 meters. The eight channels of the imagery correspond to the following wavelengths:

\begin{tabular} {c c}
    \parbox{5cm}{
        \begin{itemize}
            \item Coastal: 400 - 450 nm 			
            \item Blue: 450 - 510 nm			
            \item Green: 510 - 580 nm 			
            \item Yellow: 585 - 625 nm
        \end{itemize}
    }
    \parbox{5cm}{
        \begin{enumerate} 			
            \item Red: 630 - 690 nm
            \item Red Edge: 705 - 745 nm
            \item Near-IR1: 770 - 895 nm
            \item Near-IR2: 860 - 1040 nm
        \end{enumerate}
    }
\end{tabular}
\bigskip

Three of those channels were extracted and used as RGB data. 
Each pixel of an image is described by three bytes representing the intensity of the wavelengths of the 
reflected light corresponding to either red, green or blue.

The DSM data was collected using light detection and ranging technology (Lidar) which measures the 
distance to points of earths surface. This distance is proportional to the value of the single channel
that each pixel of the DSM has.

Lastly there are semantic labeled pictures with one channel of a single byte that encodes one of five 
different topographic classes. Those classes are vegetation, water, ground, building and clutter. 
The semantic labeling was done automatically from lidar data but manually checked and corrected afterwards.

Those four categories of data all cover one square kilometer in each image.
Additionally they contain a lot of oblique view on buildings and other valuable
features like clear shadows making the data ideal for training models that should 
detect those features.

In the dataset there are $2,783$ $1024\times 1024$ RGB images available. Since the autoencoder should
be able to distinguish between categories like shadows and vegetation each image is split into $64$
$128\times 128$ pictures resulting in $178,112$ total training images. Those smaller picture sections
are more likely to have a dominant feature like containing mainly shadows or only vegetation.

\subsection{Architecture} \label{architecture}

The first general structure of the vanilla autoencoder and the variational autoencoder is a combination 
of the convolutional autoencoder and the variational autoencoder presented in the second edition of
Hans-On Machine Learning \parencite{2017-geron-homl}.
That model is adjusted to work with $128\times 128$ RGB images

\subsubsection{The Loss Function}

Intuitively the loss function defines a goal that the model should reach when training by minimizing
the loss function. In the variational autoencoder one of those goals is that the distribution of the 
latent space is similar to a normal distribution since that makes it possible to sample latent 
variables from a normal distribution. From those sampled variables the decoder can generate an output
that resembles the real distribution.

To define that goal in a loss function Kullback-Leibler divergence is used to force the distribution
of the latent space to resemble a normal distribution.
%TODO D_{KL}(P\parallel Q) von latent  und normal wird minimiert

The second goal is of course that the output of the variational autoencoder is similar to the input.
The loss function used for this purpose is the sum of the absolute differences between each input and
each output pixel.

The final loss function is the sum of both, the Kullback-Leibler divergence and the sum of the absolute
errors.

\subsection{Understanding Latent Space}

To understand and work with the latent space learned by the variational autoencoder is problematic and
difficult since it is a very high dimensional space and it is not known what information of the input the
autoencoder learns. For the understanding and visualization of the latent space its dimensionality needs
to be reduced while preserving the underlying structure of the latent space. Then the low dimensional
codes can be visualized in a way that allows humans to comprehend what the autoencoder learned.
One dimensionality reduction method that could be used here is principal component analysis (PCA).

However,
t-distributed stochastic neighbor embedding (t-SNE), as presented in \ref{t-sne}, is the technique used in this work.
The reason for this choice is that PCA focuses on preserving large distances, so dissimilar points in the
high dimensional space appear far apart in a low dimensionality visualization. For visualization though
it may be more interesting to keep more of the underlying structure which is accomplished by t-SNE as its
focus is on preserving the context of points to their neighbors \parencite{2008-vanDerMaaten-visualizing}.
That behavior is caused by the asymmetry of the used Kullback-Leibler divergence \ref{KL-Divergence}.
In the training process it penalizes a lot if large probabilities, i. e. far apart points, in high dimensional
space are represented by small probabilities, i. e. close together points, in low dimensional space. Meanwhile, the 
penalty is little if small probabilities in high dimensional space are represented by large probabilities in
low dimensional space, leading to the claimed focus on local similarities.

After t-SNE is used to reduce the dimensions of the latent code to 2, the low dimensional representation can be
plotted in a scatter plot. That way it becomes evident if the autoencoder has learned any distinct clusters.
To recap, every point in those plots is a low dimensional representation of a latent code that corresponds to
a $128 \times 128$ section of an RGB satellite image. 
Now the goal is to find out what features the learned clusters in the plots correspond to. For that purpose the
semantic labeling and digital surface models \ref{datasets} are used. In the first type of plot the points
are colored according to the most dominant class in the corresponding image. Those classes are building, vegetation,
water, ground and clutter. In the second kind of plot the points have a gray value equal to the average pixel
values in the digital surface models meaning that points for images, that have on average more heights, are brighter
than points representing images with less heights.
In both types of plots the points are squares if the related image is from Jacksonville or circles if it is from
Omaha.

In those plots it can be observed if the clusters correlate to one of the visualized features which would mean that
the autoencoder has learned to cluster that feature.
This method is repeated for different coding sizes for the two architectures presented in \ref{architecture}.
That process gives insight into what coding sizes yield the best unsupervised clustering. The specific experiments
and results are found in the experiments section \ref{latent_space_experiments}.
