Explained in own words:


Back Propagation:
    (completely false)
    The result of a neural network for one piece of training data can be compared with the desired result(which we know in supervised learning) and then you can determine how big the difference is.
    You express that difference in an error function and try to minimize that function.
    To do that we take the derivative of that function for each variable of the neural network and end up with a large gradient vector.
    Then we try to find a vector x that when inserted into the gradient vector results in a vector of zeros. That is done by appyling the newton method to each entry of the vector.
    (what prevents us from finding local maxima instead of minima?)
    After we found the right x values we change all the weights of the neural network a little bit into the direction of those x values.
    (new Assumption)
    Back Propagation/Gradient Descent means to compute the gradients (in practice often done numerically) of a function for each parameter.
    Then you subtract the gradient multiplied with a learning rate from its corresponding parameter to the parameter a bit into the direction of the optimum
Convolutional Neural Network:
    Convolutional Nueral Networks are the same as Neural Networks with the difference that input nodes that are in some way close to each other are grouped together and
    they all are connected to a single node in the next layer. This is repeated for each layer until the desired output size is reached.
Variatonal Autoencoder:
    The difference between an autoencoder and a variational autoencoder is that the encoder generates a mean and a standard deviation and the actual encoding is a sample of a normal distribution with that mean and that standard deviation.
    This brings the benefit that an actually incredibly complicated distribution now could look like it were sampled from a normal distribution.
    This way you can just sample a coding from a normal distribution and use it to generate a result with the decoder that resembles the actual distribution.
    The encoder is basically forced to put more understanding into the encoding.
Denoising Autoencoder:

Generative Adversarial Network:




Assumptions:
    1.  The main advantage of a CNN over a regular NN is that it is computationally faster since not every node of each layer is connected to every node of the next layer.
        CNNs are better than regular NNs at extracting knowledge of small areas of pictures since they group the inputs of small areas to generate the value of a node.
    2.  (probably false) The main application of autoencoders is to use the decoder as a generator that creates new images the from a correctly manipulated input encoding.
        2.1 The main value of autoencoders lies in the generation of the encoding and the decoder is just a byproduct.
            We want to force the encoder to learn the desired knowledge by constraining it. For example if the encoding has a smaller dimension than the input
            the encoder is forced to only put the most important features into the latent code. This way the encoder has learnt what the most important features of an image are and it learnt to find the underlying patterns which is obviously desired. 
            (could be used for pretraining a net for classification)
